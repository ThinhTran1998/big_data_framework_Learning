{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f402b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73979f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc2f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv('./data_sample/Books.csv')\n",
    "\n",
    "rdd = spark.sparkContext.textFile('./data_sample/Books.csv')\n",
    "\n",
    "print(type(df))\n",
    "print(type(df.rdd))\n",
    "print(type(rdd))\n",
    "print(type(rdd.map(lambda x: (x, )).toDF()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9243c13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['ISBN;Title;Author;Year;Publisher']\n",
      "\n",
      "\n",
      "<class 'str'>\n",
      "ISBN;Title;Author;Year;Publisher\n"
     ]
    }
   ],
   "source": [
    "print(type(rdd.take(1)))\n",
    "print(rdd.take(1))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(type(rdd.first()))\n",
    "print(rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46f25984",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_1993 = df.rdd.filter(lambda x: x['Year'] is not None).filter(lambda x: '1993' in x['Year'])\n",
    "out_data = book_1993.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4a52078",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data_frame = spark.createDataFrame(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94ed0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data_frame.write.mode(\"overwrite\").options(header='true').csv(\"hdfs://localhost:9000/sample_Thinh/book_1993.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730cb5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a259f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcade0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/create-your-first-etl-pipeline-in-apache-spark-and-python-ec3d12e2c169\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20baefcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976f746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb404d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
